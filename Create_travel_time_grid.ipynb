{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create travel time grid with Pykonal \n",
    "more info in original publication : DOI: 10.1785/0220190318 <br>\n",
    "https://www.researchgate.net/publication/341889419_PyKonal_A_Python_Package_for_Solving_the_Eikonal_Equation_in_Spherical_and_Cartesian_Coordinates_Using_the_Fast_Marching_Method <br>\n",
    "Pykonal documentation : https://malcolmw.github.io/pykonal-docs/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard data handling \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# create progress bar \n",
    "from tqdm import tqdm\n",
    "# plot\n",
    "from matplotlib import pyplot as plt\n",
    "# Convert UTM to other projection\n",
    "import utm\n",
    "# Ray tracing tool \n",
    "import pykonal\n",
    "from pykonal.transformations import geo2sph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load Velocity model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Velocity model dimensions <br>\n",
    "depth : in positive number of km (ie 5 km depth = 5, 200m above sea level = -0.2)<br>\n",
    "sea level = earth radius 6371 km <br>\n",
    "latitude : ° N <br>\n",
    "longitude : in ° E\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D model from .CSV file \n",
    "Projection : spherical  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_velocity_model = r''\n",
    "\n",
    "df = pd.read_csv(path_velocity_model, sep = ',')\n",
    "\n",
    "# Spherical projection \n",
    "df = df.rename(columns={'Z(m)': \"depth\", 'V(km/s)': 'Vs'})\n",
    "lon_min = np.full(df.index.values.shape, min(df['X_UTM(m)'].values))\n",
    "lat_min = np.full(df.index.values.shape, min(df['Y_UTM(m)'].values))\n",
    "df['latitude'] = utm.to_latlon(lon_min, df['Y_UTM(m)'].values, 28,'R')[0]\n",
    "df['longitude'] = utm.to_latlon( df['X_UTM(m)'].values, lat_min, 28,'R')[1]\n",
    "\n",
    "# get epth in km \n",
    "df['depth'] = -df['depth']/1000\n",
    "\n",
    "# Convert DataFrame to an xarray Dataset\n",
    "ds = df.set_index(['latitude','longitude' ,'depth' ]).to_xarray()\n",
    "\n",
    "# Extract the velocity as a DataArray\n",
    "velocity_model = ds['Vs']\n",
    "\n",
    "# If velocity model has no values above topography \n",
    "# extrude model upward to have all station within data domain \n",
    "\n",
    "fill_limit = 6  # Maximum number of times a NaN can be replaced\n",
    "# Create a counter array to track the number of times a NaN is replace\n",
    "\n",
    "test = velocity_model.values[::, ::, ::-1]\n",
    "fill_count = np.zeros_like(test, dtype=int)\n",
    "\n",
    "for depth_slice in range(1, test.shape[2]):\n",
    "    old_slice = test[:, :, depth_slice - 1]\n",
    "    new_slice = test[:, :, depth_slice]\n",
    "    \n",
    "    # Create a mask for NaN values that have been replaced less than twice\n",
    "    mask = np.isnan(new_slice) & (fill_count[:, :, depth_slice] < fill_limit)\n",
    "    \n",
    "    # Replace values where the mask is True\n",
    "    test[:, :, depth_slice][mask] = old_slice[mask]\n",
    "    \n",
    "    # Update the fill count for those replaced values\n",
    "    fill_count[:, :, depth_slice:][mask] += 1\n",
    "\n",
    "velocity_model.values = test[::, ::, ::-1]\n",
    "\n",
    "\n",
    "velocity_model.isel(depth =5).plot()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D model from .CSV file \n",
    "projection : spherical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH_VELOCITY = r\"\"\n",
    "\n",
    "depths = np.linspace(-2, 5, 100)\n",
    "latitudes = np.linspace(63.81810, 64.03551,100)\n",
    "longitudes = np.linspace(-22.62691, -21.94679, 100)\n",
    "\n",
    "phase = 'Vs'\n",
    "\n",
    "velocity_layers = pd.read_csv(\n",
    "    FILEPATH_VELOCITY,\n",
    "    usecols=[0, 3],\n",
    "    names=[\"depth\", 'Vs'],\n",
    "    skiprows=1,\n",
    "    index_col=\"depth\",\n",
    "    )\n",
    "\n",
    "smoothing_window = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# quick visualisation\n",
    "velocity_layers.T\n",
    "\n",
    "velocity_layers_interp = velocity_layers.reindex(depths, method=\"ffill\")\n",
    "padded_data = np.pad(velocity_layers_interp[phase].values, pad_width=smoothing_window//2, mode='edge')\n",
    "\n",
    "# smoothing on model\n",
    "if smoothing_window %2 != 0:\n",
    "    velocity_layers_interp[phase] = np.convolve(padded_data, np.ones(smoothing_window)/smoothing_window, mode = 'valid')[:]\n",
    "if smoothing_window %2 == 0:\n",
    "    velocity_layers_interp[phase] = np.convolve(padded_data, np.ones(smoothing_window)/smoothing_window, mode = 'valid')[:-1]\n",
    "velocity_layers.plot(drawstyle=\"steps-post\")\n",
    "velocity_layers_interp.sort_values(\"depth\").plot(\n",
    "    drawstyle=\"steps-post\",\n",
    "    xlabel=\"Depth (km)\",\n",
    "    ylabel=\"Speed (km/s)\",\n",
    "    title=\"1D velocity model\",\n",
    "    ax=plt.gca(),\n",
    "    grid=True,\n",
    "    figsize=(7.5, 6),\n",
    "    marker=\"s\",\n",
    "    ls=\"\"\n",
    ")\n",
    "plt.axvspan(depths.min(), depths.max(), alpha=0.2) # Labels and legends\n",
    "#plt.legend([\"P\", \"S\", \"P interpolated\", \"S interpolated\", \"Domain\"])\n",
    "plt.show()\n",
    "#### La Palma\n",
    "\n",
    "velocities = velocity_layers_interp.stack().to_xarray()\n",
    "velocities = velocities.rename({\"level_1\": \"phase\"})\n",
    "velocities = velocities.sel(phase = phase)\n",
    "velocity_model = velocities.expand_dims(latitude=latitudes, longitude=longitudes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homogeneous velocity model\n",
    "projection : spherical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1 # km/s\n",
    "\n",
    "depths = np.linspace(-2, 5, 100)\n",
    "latitudes = np.linspace(63.85923, 63.91461,100)\n",
    "longitudes = np.linspace(-22.33035, -22.20656, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D array filled with the value C\n",
    "data = np.full((len(depths), len(latitudes), len(longitudes)), C)\n",
    "\n",
    "# Create the DataArray\n",
    "velocity_model = xr.DataArray(\n",
    "    data,\n",
    "    coords={\n",
    "        \"depth\": depths,\n",
    "        \"latitude\": latitudes,\n",
    "        \"longitude\": longitudes\n",
    "    },\n",
    "    dims=[\"depth\", \"latitude\", \"longitude\"],\n",
    "    name=\"velocity\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Resample Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insure that the model has homogeneous spacing along dimension. <br> \n",
    "Increasing the resolution can give better results. The travel time grid can be downsampeled later in the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### resample velocity grid ###\n",
    "n_lat_sample = 100 # number of lat sample for new grid \n",
    "n_lon_sample = 101 # number of lon sample for new grid \n",
    "n_dep_sample = 102 # number of sample for new depth grid \n",
    "\n",
    "lat_ratio = velocity_model.coords['latitude'].shape[0]/n_lat_sample\n",
    "lon_ratio = velocity_model.coords['longitude'].shape[0]/n_lon_sample\n",
    "dep_ratio = velocity_model.coords['depth'].shape[0]/n_dep_sample\n",
    "\n",
    "### CORE ###\n",
    "new_lat = np.linspace(velocity_model.coords['latitude'][0], velocity_model.coords['latitude'][-1], n_lat_sample)\n",
    "new_lon = np.linspace(velocity_model.coords['longitude'][0], velocity_model.coords['longitude'][-1], n_lon_sample)\n",
    "new_dep = np.linspace(velocity_model.coords['depth'][0], velocity_model.coords['depth'][-1], n_dep_sample)\n",
    "\n",
    "velocity_model = velocity_model.interp(latitude = new_lat, longitude = new_lon, depth = new_dep, method='linear')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orient velocity data so when we extract them in an array to create traveltime grid they are not flipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orientation of the grid \n",
    "# The reference point (corner top left at the bottom of the grid) is a max or a min value of the axis? \n",
    "# Allows to give correct orientation of the grid and avoid flipping during computation.\n",
    "\n",
    "\n",
    "ref_lon = 'min'\n",
    "ref_lat = 'max'\n",
    "ref_depth = 'max'\n",
    "\n",
    "# Ensure coordinates are in descending order\n",
    "if velocity_model.latitude.values[0] > velocity_model.latitude.values[-1] and ref_lat == 'min' :\n",
    "    velocity_model = velocity_model.sortby(\"latitude\", ascending=True)\n",
    "if velocity_model.latitude.values[0] < velocity_model.latitude.values[-1] and ref_lat == 'max' :\n",
    "    velocity_model = velocity_model.sortby(\"latitude\", ascending=False)\n",
    "\n",
    "if velocity_model.longitude.values[0] > velocity_model.longitude.values[-1] and ref_lon == 'min' :\n",
    "    velocity_model = velocity_model.sortby(\"longitude\", ascending=True)\n",
    "if velocity_model.longitude.values[0] < velocity_model.longitude.values[-1] and ref_lon == 'max' :\n",
    "    velocity_model = velocity_model.sortby(\"longitude\", ascending=False)\n",
    "\n",
    "if velocity_model.depth.values[0] > velocity_model.depth.values[-1] and ref_depth == 'min' :\n",
    "    velocity_model = velocity_model.sortby(\"depth\", ascending=True)\n",
    "if velocity_model.depth.values[0] < velocity_model.depth.values[-1] and ref_depth == 'max' :\n",
    "    velocity_model = velocity_model.sortby(\"depth\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick visualisation to see if it make sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_model.isel(latitude =25).plot(x = 'longitude')\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Load station information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change STATION_ENTRIES to match your CSV header <br>\n",
    "The depth of your station must be in km with 0 at sea level. Negative depth values are positive altitude (i.e. depth = -2 means the staion is at 2km above sea level)<br>\n",
    "The index of the pandas dataframe must be the station name (it will be used to name the traveltime grid in the xr.Dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### path to station info ###\n",
    "station_info_path = r'C:\\Users\\liliv\\Desktop\\Fagradalfjall\\Tremor_loc\\Velocity model\\station_fagra.csv'\n",
    "station_info = pd.read_csv(station_info_path, sep = ',', index_col =0)\n",
    "print(station_info.tail())\n",
    "STATION_ENTRIES = [\"lat\", \"lon\", \"alt\"] # order = lat, lon, dep\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Calculate travel time SPHERICAL \n",
    "!!! Velocity model with SPHERICAL coordinates !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues and trouble shooting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1) the model run but the location of the station (min travel time) is not correct. <br>\n",
    "This can be caused by the traveltime gid beeing extracted from the xarray in the wrong orientation. Must verify that step 2 is well filled <br>\n",
    "i.e. is the reference point a min or a max in lat, lon depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) if all values are np.inf in travel time grid <br>it could be caused by the source being defined outside of the model's grid (verify of source parameters, the classic is depth express in meters when pykonals takes km and / or grid dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. To verify that all parameters are set correctly, one can run a grid with velocities of 1km/s with a source at the grid origin: <br>\n",
    "solver.src_loc = reference_point <br>\n",
    "The time in second at the edges should equal the dimensions in km of the grid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model dimensions and reference points\n",
    "==> reference point = lowest, northest, eastest point of the model. <br>\n",
    "==> node interval = distance between adjacent  nodes for: depth from sea level (km), latitude (radians), longitude (radians) [sperical model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE MODEL REFERENCE ### \n",
    "\n",
    "# ==>   reference_point = lowest, northest, eastest point of the model.\n",
    "# ==>   depth, lat, lon =  geo2sph((lat, lon, depth)).\n",
    "\n",
    "latitudes = velocity_model.coords['latitude'].values\n",
    "longitudes = velocity_model.coords['longitude'].values\n",
    "depths = velocity_model.coords['depth'].values\n",
    "reference_point = geo2sph((latitudes.max(), longitudes.min(), depths.max()))\n",
    "print(reference_point)\n",
    "\n",
    "# ==> node_interval = distance between adjacent  nodes for: depth (m), latitude (radians), longitude (radians).\n",
    "# ==> The units for nodal distance are m,rad,rad for spherical model but becoms m, m, m for carteian model\n",
    "# ==> !!! lat and lon in radians because model in spherical coordinates (mandatory for PointSourceSolver) otherwise can create a cartesian model, then units are not the same.\n",
    "\n",
    "node_intervals = (\n",
    "    np.abs(depths[1] - depths[0]),\n",
    "    np.deg2rad(np.abs(latitudes[1] - latitudes[0])),\n",
    "    np.deg2rad(np.abs(longitudes[1] - longitudes[0])),\n",
    ")\n",
    "print(node_intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose velocity model to go from lat lon depth to depth lat lon\n",
    "velocities = velocity_model.transpose( 'depth','latitude', 'longitude').copy()\n",
    "# container to keep travel time grids \n",
    "ttime_ds = xr.Dataset( coords=velocity_model.coords)\n",
    "\n",
    "for Station_Name in tqdm(station_info.index):\n",
    "\n",
    "    ### Create and run model ###\n",
    "    solver = pykonal.solver.PointSourceSolver(coord_sys=\"spherical\")\n",
    "    solver.velocity.min_coords = reference_point\n",
    "    solver.velocity.node_intervals = node_intervals\n",
    "    solver.velocity.npts = velocities.values.shape\n",
    "    solver.velocity.values = velocities.values\n",
    "\n",
    "    # Initialize the source location with a random location within the\n",
    "    # computational grid.\n",
    "    # Source\n",
    "    src_loc = station_info.loc[Station_Name][STATION_ENTRIES].values\n",
    "    solver.src_loc = np.array(geo2sph(src_loc).squeeze()) # \n",
    "\n",
    "    # Compute traveltimes.\n",
    "    solver.solve()\n",
    "\n",
    "    tt = solver.tt.values\n",
    "\n",
    "    travel_times = velocities.copy()\n",
    "    travel_times.values  = tt\n",
    "    travel_times = travel_times.transpose('latitude', 'longitude', 'depth')\n",
    "\n",
    "    values = travel_times.values\n",
    "    values = np.where(np.abs(values) > 1e10, np.nan, values)\n",
    "    values[np.isinf(values)] = np.nan\n",
    "    data_array = xr.DataArray(values, dims = travel_times.dims, coords=travel_times.coords)\n",
    "    ttime_ds[str(Station_Name)] = data_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick visualisation to see if it make sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change slice index in isel() to moove them in the grid \n",
    "station = ''\n",
    "\n",
    "ttime_ds[station].isel(latitude = 27).plot(x = 'longitude')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "ttime_ds[station].isel(longitude = 25).plot(x = 'latitude')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "ttime_ds[station].isel(depth = 25).plot(x = 'longitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Save ttime grids in h5 format "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resample the travel time grid to reduce computational time for the task that uses the grid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### resample travel time grid before saving if needed ###\n",
    "\n",
    "\n",
    "n_lat_sample = 100 # number of lat sample for new grid \n",
    "n_lon_sample = 101 # number of lon sample for new grid \n",
    "n_dep_sample = 102 # number of sample for new depth grid \n",
    "\n",
    "lat_ratio = ttime_ds.coords['latitude'].shape[0]/n_lat_sample\n",
    "lon_ratio = ttime_ds.coords['longitude'].shape[0]/n_lon_sample\n",
    "dep_ratio = ttime_ds.coords['depth'].shape[0]/n_dep_sample\n",
    "\n",
    "### CORE ###\n",
    "new_lat = np.linspace(ttime_ds.coords['latitude'][0], ttime_ds.coords['latitude'][-1], n_lat_sample)\n",
    "new_lon = np.linspace(ttime_ds.coords['longitude'][0], ttime_ds.coords['longitude'][-1], n_lon_sample)\n",
    "new_dep = np.linspace(ttime_ds.coords['depth'][0], ttime_ds.coords['depth'][-1], n_dep_sample)\n",
    "\n",
    "ttime_ds = ttime_ds.interp(latitude = new_lat, longitude = new_lon, depth = new_dep, method='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only a subset of the map to reduce next step computational cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### SUBSET ######################\n",
    "\n",
    "# Suppose your dataset is called ds and the variable is \"traveltime\"\n",
    "depth_new = np.linspace(-2, 1, 30)\n",
    "lat_new = np.linspace(63.85923, 63.91461,50)\n",
    "lon_new = np.linspace(-22.33035, -22.20656, 50)\n",
    "\n",
    "# Build the target grid\n",
    "target_grid = {\n",
    "    \"latitude\": lat_new,\n",
    "    \"longitude\": lon_new,\n",
    "    \"depth\": depth_new\n",
    "}\n",
    "\n",
    "# Interpolate onto the target grid\n",
    "ttime_ds = ttime_ds.interp(target_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as a dataset containing all travel time grids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_folder= r''\n",
    "file_name = ''\n",
    "\n",
    "ttime_ds.to_netcdf(saving_folder+'\\\\'+file_name+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save each travel time grid in a separte dataset (usefull when many high resolution grids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in ttime_ds.data_vars:\n",
    "    da = ds[var_name]             \n",
    "    da.to_netcdf(saving_folder+'\\\\'+var_name+'.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykonal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
